[
  {
    "abstract": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA37840401\"><p>A central processing unit (CPU) of a computer and a method for reducing memory latencies in a computer memory hierarchy are described. The CPU includes an external cache controller and a primary memory controller. An instruction buffer in the primary memory controller stores an address from a primary memory page corresponding to a previous address request. A comparator circuit of the primary memory controller is used to compare a present address request corresponding to an instruction cache miss signal to the address stored in the instruction buffer. If an instruction buffer hit is achieved, memory latencies associated with the external cache controller and the primary memory controller are avoided. If an instruction buffer miss is experienced, the primary memory controller, under predetermined conditions, stores, in the instruction buffer, an address following an address corresponding to data from a primary memory page specified by the present address request. This operation frequently results in the instruction buffer storing early, i.e., prefetching, an address request that may be subsequently called by a computer program. When this is achieved, an address request may be rapidly retrieved without incurring the memory latency overhead of the external cache controller and the primary memory controller. The predetermined conditions may include that the address request corresponds to an instruction, and that an address follow signal and a memory controller free signal are generated. In alternative embodiments, the instruction buffer may be checked for a miss before or after the external cache is checked for a cache miss.</p></abstract>",
    "assignees": "SUN MICROSYSTEMS, INC., ",
    "inventors": "Rajasekhar Cherabuddi",
    "patent_number": "US-5835947-A",
    "publication_date": "1998-11-10",
    "title": "OF A COMPUTER"
  },
  {
    "abstract": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA36882696\"><p>A computer (20) is configured for optimizing the processing rate of instructions and the throughput of data. The computer (20) includes a main memory (99), a memory control unit (22), a physical cache unit (100), and a central processor (156). A instruction processing unit (126) is included within the central processor (156). The function of the instruction processing unit (126) is to decode instructions and produce instruction execution commands or directing the execution of the instructions within the central processor (156). Instructions are transferred from the main memory (99) into a register (180) where the address fields of the instructions are decoded to produce a cracked instruction and these instructions are stored in a logical instruction cache (210). As the cracked instructions are selected they are transferred to an output buffer and decoder (214) where the remaining fields of the instructions are decoded to produce instruction execution commands. The instructions in the cache (210) are stored at logical rather than at physical addresses. The cache (210) further can operate at double the rate of a basic clock period for the computer (20) such that a branch instruction can be selected in one clock cycle. The combination of the logical instructiion cache (210) and the concurrent computation of program counts serves to substantially increase the instruction execution rate for the computer (20).</p></abstract>",
    "assignees": "CONVEX COMPUTER CORPORATION, ",
    "inventors": "Michael C. Harris, David M. Chastain, Gary B. Gostin",
    "patent_number": "US-4873629-A",
    "publication_date": "1989-10-10",
    "title": "Instruction processing unit for computer"
  },
  {
    "abstract": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA10224446\" source=\"national office\"><p>A CPU module has a processor, cache memory, cache controller, and system interface attached to a processor bus. The system interface is attached to a system bus shared by memory, I/O, and other CPU modules. The cache controller requests control of the processor bus from the processor, and grants control to the system interface. The system interface uses the processor bus to store fill data obtained from memory into the cache in response to a read miss. The system interface also monitors system bus traffic and forwards the addresses of cache blocks to be invalidated to the cache controller over an invalidate bus. The cache controller requests control of the processor bus during a read miss to perform invalidates and writebacks. The processor grants control to the cache controller before the read miss completes, enabling the cache controller to proceed, and then re-issues the read. A protocol between the cache controller and the system interface ensures that cache fills, invalidates, and writebacks are done in the correct order to maintain data coherency. As part of this protocol, the cache controller decides when the system interface may proceed with a fill, and grants the processor bus to the system interface accordingly.</p></abstract>",
    "assignees": "DIGITAL EQUIPMENT CORPORATION, , ",
    "inventors": "Michael A. Callander, Douglas E. Sanders",
    "patent_number": "US-5276852-A",
    "publication_date": "1994-01-04",
    "title": "Method and apparatus for controlling a processor bus used by multiple processor components during writeback cache transactions"
  },
  {
    "abstract": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11192126\" source=\"national office\"><p>A computer processor has an I-unit (instruction unit) and instruction decoder, an E-unit (execution unit), a Buffer Control Element (BCE) containing a unified two-way interleaved L1 cache and providing write control to said two-way interleaved L1 cache. The processor has Double Word wide execution dataflow. An instruction decoder receiving instruction data from a unified cache before decoding causes, for stores, I-unit logic to initiate a request ahead of execution to tell the buffer control element that stores will be made from the E-unit, and E-unit logic sends a store request to initiate a store after decoding corresponding instruction data which indicates what address in the cache the DoubleWord data is to be stored to. In the process, E-unit logic calculates, from source and destination address information address ranges information in an instruction, whether a corresponding multi-Double Word store with same byte data will result from the data patterns, and, when a multi-Double Word store could result, it enables the E-unit to request the writing of an incoming Double Word on the computer's data bus for both Double Word L1 cache interleaves using the same address for both to effectively write two consecutively addressed DoubleWords for the same cycle to achieve a Quad Word store in a cycle.</p></abstract>",
    "assignees": "INTERNATIONAL BUSINESS MACHINES CORPORATION, ",
    "inventors": "Chung-Lung Kevin Shum, Wen He Li, Charles Franklin Webb",
    "patent_number": "US-6233655-B1",
    "publication_date": "2001-05-15",
    "title": "Method for Quad-word Storing into 2-way interleaved L1 cache"
  },
  {
    "abstract": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA9903057\" source=\"translation\"><p>A microprocessor architecture is disclosed having separate very high speed instruction and data interface circuitry for coupling via respective separate very high speed instruction and data interface buses to respective external instruction cache and data cache circuitry. The microprocessor is comprised of an instruction interface, a data interface, and an execution unit. The instruction interface controls communications with the external instruction cache and couples the instructions from the instruction cache to the microprocessor at very high speed. The data interface controls communications with the external data cache and communicates data bidirectionally at very high speed between the data cache and the microprocessor. The execution unit selectively processes the data received via the data interface from the data cache responsive to the execution unit decoding and executing a respective one of the instructions received via the instruction interface from the instruction cache. In one embodiment, the external instruction cache is comprised of a program counter and addressable memory for outputting stored instructions responsive to its program counter and to an instruction cache advance signal output from the instruction interface. Circuitry in the instruction interface selectively outputs an initial instruction address for storage in the instruction cache program counter responsive to a context switch or branch, such that the instruction interface repetitively couples a plurality of instructions from the instruction cache to the microprocessor responsive to the cache advance signal, independent of and without the need for any intermediate or further address output from the instruction interface to the instruction cache except upon the occurrence of another context switch or branch.</p></abstract>",
    "assignees": "INTERGRAPH CORPORATION, , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ",
    "inventors": "Howard G. Sachs, James Y. Cho, Walter H. Hollingsworth",
    "patent_number": "US-4884197-A",
    "publication_date": "1989-11-28",
    "title": "Method and apparatus for addressing a cache memory"
  },
  {
    "abstract": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA11751851\" source=\"national office\"><p>The present invention relates to the architecture and use of a computer system optimized for the efficient modeling of graphics. The computer system has a primary processor and a graphics processor. The primary processor has two vector processor units within it, one which is closely connected to central processor unit. Simultaneously performing complex modeling calculations on the first vector processor and CPU, and geometry transformation calculations on the second vector processor, allows for efficient modeling of graphics. Furthermore, the graphics processor is optimized to rapidly switch between data flow from the two vector processors. In addition, the graphics processor is able to render many pixels simultaneously, and has a local memory on the graphics processor chip that acts as a frame buffer, texture buffer, and z buffer. This allows a high fill rate to the frame buffer.</p></abstract>",
    "assignees": "SONY COMPUTER ENTERTAINMENT INC., , , ",
    "inventors": "Masakazu Suzuoki, Akio Ohba, Masaaki Oka, Toshiyuki Hiroi, Teiji Yutaka, Toyoshi Okada, Masayoshi Tanaka",
    "patent_number": "US-6807620-B1",
    "publication_date": "2004-10-19",
    "title": "Game system with graphics processor"
  },
  {
    "abstract": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA10900538\" source=\"national office\"><p>A computer system employing an instruction tracing mechanism includes a memory and a CPU. The memory includes a trace buffer used to store records of the instruction tracing. Entries in the trace buffer are pointed to by a tracer pointer. The memory is coupled to the CPU via a bus interface unit. Instructions are passed from the memory to the CPU via the bus interface unit and are queued in an instruction cache, which includes a byte queue. The CPU further includes a control unit coupled to a control register, the tracer pointer and a maskable ROM. The control unit controls and activates the instruction tracing mechanism. In response to software sitting a bit in the control register, the control unit functions to retrieve a special tracing microcode sequence from MROM to provide a trace record of the instructions as they are passed to the instruction decoders. A counting mechanism may also be activated to count instructions and store the count in the memory. The computer system further includes a register file coupled to the instruction decoders and functional units, also coupled to instruction decoders, to receive and process the instructions.</p></abstract>",
    "assignees": "ADVANCED MICRO DEVICES, INC., ",
    "inventors": "David S. Christie",
    "patent_number": "US-5944841-A",
    "publication_date": "1999-08-31",
    "title": "Microprocessor with built-in instruction tracing capability"
  },
  {
    "abstract": "<abstract lang=\"EN\" load-source=\"docdb\" mxw-id=\"PA10151824\" source=\"translation\"><p>A microcomputer is disclosed which is specifically designed for computation-intensive applications. The microcomputer contains on-chip RAM and ROM, and has peripheral ports for access of external memory and input/output functions. The microcomputer has a central processing unit with a floating-point multiplier in parallel with an arithmetic logic unit, and uses a plurality of registers as multiple accumulators. The central processing unit further contains two auxiliary arithmetic logic units, in parallel with one another, and which are each connected to a set of address lines in a memory bus; the two auxiliary arithmetic logic units thus generate two separate memory addresses in parallel. The memory bus also contains one set of data lines, connected to the RAM and ROM, and to the central processing unit. The on-chip RAM and ROM are responsive to the two sets of address lines in time-multiplexed fashion to provide memory access via data lines twice per system clock cycle. A second memory bus is also connected to the on-chip RAM and ROM, and to the peripheral ports, so that access to one of the memory elements via said first memory bus can occur simultaneously with, and independently from, access to another of said memory elements via said second memory bus. The on-chip memory and external memory are all mapped into a single memory address space, which allows simultaneous program and data fetches via the two memory buses, or a program and data fetch during the same cycle using the first time-multiplexed bus. Memory-mapped input and output functions are performed by on-chip peripherals, which are connected to a peripheral bus connected to one of the peripheral ports of the microcomputer. The peripheral bus allows for substantial flexibility relative to the configuration of the microcomputer.</p></abstract>",
    "assignees": "TEXAS INSTRUMENTS INCORPORATED",
    "inventors": "Jerald G. Leach, L. Ray Simar, Jr.",
    "patent_number": "US-5179689-A",
    "publication_date": "1993-01-12",
    "title": "Dataprocessing device with instruction cache"
  },
  {
    "abstract": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA36487663\"><p>A virtual storage data processing system having an address translation unit shared by a plurality of processors, located in a memory control unit connected to a main memory is disclosed. One of the plurality of processors is a job processor which accesses the main memory with a virtual address to execute an instruction and includes a cache memory which is accessed with a virtual address. One of the plurality of processors is a file processor which accesses the main memory with a virtual address to transfer data between the main memory and an external memory. The cache memory receives the virtual address when the file processor writes to the main memory and if it contains a data block corresponding to the virtual address, it invalidates the corresponding data block. The address translation unit translates the address differently for the access from the file processor and the accesses from other processors.</p></abstract>",
    "assignees": "HITACHI, LTD., HITACHI ENGINEERING CO., LTD., , ",
    "inventors": "Yasushi Fukunaga, Tadaaki Bandoh, Hidekazu Matsumoto, Ryosei Hiraoka, Jushi Ide, Takeshi Kato, Tetsuya Kawakami",
    "patent_number": "US-4481573-A",
    "publication_date": "1984-11-06",
    "title": "Shared virtual address translation unit for a multiprocessor system"
  },
  {
    "abstract": "<abstract lang=\"EN\" load-source=\"patent-office\" mxw-id=\"PA37611390\"><p>A Harvard architecture data processing system includes a processor, main memory, an instruction cache, and a data cache. As is generally known with the Harvard architecture, these components are interconnected by an instruction bus, an instruction address bus, a data bus, and a data address bus. The instruction cache includes a branch target section and a general instruction section. For each instruction request by the processor, both sections are examined to determine if the requested instruction is in the cache. If it is, it is transmitted from the cache to the processor. If it is not, an instruction line including the requested instruction is fetched from main memory. If the requested instruction represents a jump (the result of an unconditional branch or a conditional branch the condition of which is met) the fetched instruction line can be stored only in the branch target section. If the requested instruction is simply the one located at the address one above that of the previous instruction, the fetched instruction line can only be stored in the general instruction section. This approach preserves branch targets in cache, while allowing all cached instructions to be available to the processor irrespective of whether a jump is called for.</p></abstract>",
    "assignees": "VLSI TECHNOLOGY, INC., , , , , , , , , ",
    "inventors": "Kenneth A. Dockser",
    "patent_number": "US-5603045-A",
    "publication_date": "1997-02-11",
    "title": "Microprocessor system having instruction cache with reserved branch target section"
  }
]
